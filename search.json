[
  {
    "objectID": "index.html#initial-example",
    "href": "index.html#initial-example",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Initial example",
    "text": "Initial example\n\nIs this a good answer ? Hard to tell"
  },
  {
    "objectID": "index.html#initial-example-1",
    "href": "index.html#initial-example-1",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Initial example",
    "text": "Initial example\n\n\n\n\n\nAnswer from ChatGPT\n\n\n\n\n\n\nAnswer from Google\n\n\n\nBoth answers are better: precise, contextual"
  },
  {
    "objectID": "index.html#why-rag",
    "href": "index.html#why-rag",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Why RAG ?",
    "text": "Why RAG ?\n\nRetrieval-Augmented Generation (RAG) combines:\n\nInformation retrieval from a knowledge base\nText generation using aLLM with contextualized information\n\n\n\n\n\n\n\n\nObjective:\n\n\n\nProduce accurate information (no hallucination)\nProduce verifiable information (source citation)\nPropose up-to-date answer\nInterpretation of the meaning of the question, unlike traditional text queries based on bags of words"
  },
  {
    "objectID": "index.html#why-doing-that",
    "href": "index.html#why-doing-that",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Why doing that ?",
    "text": "Why doing that ?\n\nAsking Google is great but user needs to have good keywords\n\nAssume user knows what she wants…\n… and have some literacy\n\n\n\n\nLLMs are more and more used as search engine\n\nHow can we best structure information in our website for the response to be relevant ?\n\n\n\n\n\nWe have 20+ years of experience in understanding how Google works,\n\nWe also need to understand how LLMs work\nExperimenting RAG is a good way for that"
  },
  {
    "objectID": "index.html#typical-rag-pipeline-1",
    "href": "index.html#typical-rag-pipeline-1",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Typical RAG pipeline",
    "text": "Typical RAG pipeline"
  },
  {
    "objectID": "index.html#challenge",
    "href": "index.html#challenge",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Challenge",
    "text": "Challenge\n\n\n\n\nHow should we parse the documents ?\nHow to handle tables?\nHow to handle documents metadata that can be useful ?\nShould we split the pages ?\nHow long should each chunk be ?\nHow should we chunk ?\n…"
  },
  {
    "objectID": "index.html#challenge-1",
    "href": "index.html#challenge-1",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Challenge",
    "text": "Challenge\n\n\n\nWhich embedding should I choose ?\nIs the best performing embedding in MTEB relevant for my use case ?\nWhich backend should I use for embedding ? (VLLM, Ollama…)\nWhich vector database should I use ? (ChromaDB, QDrant…)\nHow to make my vectordatabase always available to my RAG in production ?\nShould I only use semantic search or hybrid search ?\nHow many documents should I retrieve ?\nShould I rerank ? How ?\n…"
  },
  {
    "objectID": "index.html#challenge-2",
    "href": "index.html#challenge-2",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Challenge",
    "text": "Challenge\n\n\n\nWhich generative model should I use ?\nHow to prompt it to ensure context citation and avoid hallucinations ?\nHow to prompt it if there are different use cases that are covered ?\nWhich backend should I use ?\nHow to expose him to clients ? Should I expose him first to happy fews?\n…"
  },
  {
    "objectID": "index.html#rag-is-hard",
    "href": "index.html#rag-is-hard",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "RAG is hard",
    "text": "RAG is hard"
  },
  {
    "objectID": "index.html#rag-is-hard-rag-needs-evaluation",
    "href": "index.html#rag-is-hard-rag-needs-evaluation",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "RAG is hard RAG needs evaluation",
    "text": "RAG is hard RAG needs evaluation\n\nEvaluation challenges:\n\nIs the retrieved context relevant?\nIs the generation faithful to the context?\nIs the answer useful to end users (e.g. analysts, statisticians)?\n\n\n\n\nGeneric metrics are not that useful\n\nBetter to define use case related objectives\nAdapt pipelines to that end\n\n\n\n\n\nExisting plug and play frameworks show limitations\n\nTo build good RAG, need to go on details"
  },
  {
    "objectID": "index.html#many-metrics-exist",
    "href": "index.html#many-metrics-exist",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Many metrics exist",
    "text": "Many metrics exist\n\n\n\n\n\nflowchart TD\n    A[RAG Evaluation] --&gt; B[Retrieval Quality]\n    A --&gt; C[Generation Quality]\n    A --&gt; D[End-to-End Evaluation]\n    B --&gt; E[Precision, Recall, F-score,&lt;br&gt; NDCG, MRR...]\n    C --&gt; F[Accuracy, Faithfulness,&lt;br&gt; Relevance, ROUGE/BLEU/METEOR,&lt;br&gt; Hallucination...]\n    D --&gt; G[Helpfulness, Consistency,&lt;br&gt; Conciseness,&lt;br&gt; Latency, Satisfaction...]\n\n\n\n\n\n\nAn attempt to classify RAG metrics"
  },
  {
    "objectID": "index.html#they-are-not-that-much-helpful",
    "href": "index.html#they-are-not-that-much-helpful",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "They are not that much helpful",
    "text": "They are not that much helpful\n\n\n\nRAG quality depends on so many dimensions…\n… we understood Kierkegaard’s vertigo of freedom concept"
  },
  {
    "objectID": "index.html#we-need-to-know-what-we-want",
    "href": "index.html#we-need-to-know-what-we-want",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "We need to know what we want",
    "text": "We need to know what we want\n\nBest way to go forward : read Hamel Husain blog\n\nNotably: Husain (2024) and Husain (2025) posts\nPragmatic approach\n\n\n\n\nBetter to start with limited set of metrics\n\n\n\n\n\n\n\n\n“The kind of dashboard that foreshadows failure.” Husain (2025)"
  },
  {
    "objectID": "index.html#what-do-we-want",
    "href": "index.html#what-do-we-want",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "What do we want ?",
    "text": "What do we want ?\n\nNo hallucination !\n\nHow many invented references or facts ?\nHallucination rate\n\nRetrieve relevant content:\n\nDoes the retriever find the relevant page/document for a given question ?\nTopk retrieval\n\nHave a useful companion to official statistics\n\nGiven the sources, is the answer satisfactory ?\nSatisfaction rate"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Methodology",
    "text": "Methodology\n\nAround 60k pages from insee.fr\n\n\n\nEvaluating RAG on different dimensions\n\n\n\n\nMain model used:\n\nEmbedding: BAAI/bge-multilingual-gemma2\nGeneration: mistralai/Mistral-Small-24B-Instruct-2501\n\n\n\n\n\nCollected expert domain Q&A\n\nSmall sample: 62 questions\nMore questions will come later"
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Technical details",
    "text": "Technical details\n\n2 VLLMs instance (OpenAI API compatible endpoints) in backend\n\nRunning on Nvidia H100 GPU\n\nChromaDB as vector database\nLangchain for document handling\nStreamlit for front end user interface\n\n\n\n\n\n\n\nNote\n\n\nThis is a quite demanding pipeline :\n\nEmbedding and generation instances must be available at each user query"
  },
  {
    "objectID": "index.html#collect-expert-level-annotations",
    "href": "index.html#collect-expert-level-annotations",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "1. Collect expert level annotations",
    "text": "1. Collect expert level annotations\n\nTo challenge retrieval before any product launch\n\n\nOnce again Husain (2024) is right:\n\nMany frameworks can create tricky questions using LLM (RAGAS, Giskard…)\nBut nothing works better than starting from expert questions and answers in a spreadsheet\n\n\n\n\n\n\n\n\nAstuce\n\n\nCollect existing questions from insee.fr website (e.g. here) or write original ones"
  },
  {
    "objectID": "index.html#collect-expert-level-annotations-1",
    "href": "index.html#collect-expert-level-annotations-1",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "1. Collect expert level annotations",
    "text": "1. Collect expert level annotations\n\nTo challenge retrieval before any product launch"
  },
  {
    "objectID": "index.html#collect-expert-level-annotations-2",
    "href": "index.html#collect-expert-level-annotations-2",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "1. Collect expert level annotations",
    "text": "1. Collect expert level annotations\n\nTo challenge retrieval before any product launch\n\n\nHelped us to iterate over a “satisfying” strategy regarding parsing and chunking\n\nNeed medium sized chunks (around 1100 tokens)\nNot more than 1500 tokens to avoid lost in the middle\n\n\n\n\nCast the tables aside\n\nHard to chunk, hard to interpret without\nPrioretizing text content.\n\n\n\n\n\n\n\n\nNote\n\n\nThis dataset can be later on used for any parametric change in our RAG pipeline"
  },
  {
    "objectID": "index.html#collect-user-feedbacks",
    "href": "index.html#collect-user-feedbacks",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "2. Collect user feedbacks",
    "text": "2. Collect user feedbacks\n\nTo ensure we satisfy user needs\n\n\nHaving an interface gamifies the evaluation process…\n\n… which can help collecting evaluation\n\n\n\n\nWe want users to give honest feedbacks on different dimensions:\n\nSources used, quality of the answer\nFree form to understand for manual inspection to understand what does not work\nSimple feedback (👍️/👎️) to track satisfaction rate\n\n\n\n\n\n\n\n\nAstuce\n\n\nWe need good satisfaction rates before A/B testing !"
  },
  {
    "objectID": "index.html#retriever-quality",
    "href": "index.html#retriever-quality",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Retriever quality",
    "text": "Retriever quality\n\nHelped monitor retriever quality\nHelped understand problems in website parsing\n\n\ntopk = [\n  {top: 1, value: 21.31, version: \"Complete set\\nof questions\"},\n  {top: 2, value: 34.43, version: \"Complete set\\nof questions\"},\n  {top: 3, value: 37.70, version: \"Complete set\\nof questions\"},\n  {top: 4, value: 40.98, version: \"Complete set\\nof questions\"},\n  {top: 5, value: 45.90, version: \"Complete set\\nof questions\"},\n  {top: 6, value: 47.54, version: \"Complete set\\nof questions\"},\n  {top: 7, value: 49.18, version: \"Complete set\\nof questions\"},\n  {top: 8, value: 50.82, version: \"Complete set\\nof questions\"},\n  {top: 9, value: 50.82, version: \"Complete set\\nof questions\"},\n  {top: 10, value: 50.82, version: \"Complete set\\nof questions\"},\n\n  {top: 1, value: 32.35, version: \"Restricted to well\\nparsed sections\"},\n  {top: 2, value: 52.94, version: \"Restricted to well\\nparsed sections\"},\n  {top: 3, value: 58.82, version: \"Restricted to well\\nparsed sections\"},\n  {top: 4, value: 64.71, version: \"Restricted to well\\nparsed sections\"},\n  {top: 5, value: 70.59, version: \"Restricted to well\\nparsed sections\"},\n  {top: 6, value: 73.53, version: \"Restricted to well\\nparsed sections\"},\n  {top: 7, value: 76.47, version: \"Restricted to well\\nparsed sections\"},\n  {top: 8, value: 79.41, version: \"Restricted to well\\nparsed sections\"},\n  {top: 9, value: 79.41, version: \"Restricted to well\\nparsed sections\"},\n  {top: 10, value: 79.41, version: \"Restricted to well\\nparsed sections\"}\n]\n\n\n\n\n\n\n\nchart = Plot.plot({\n  y: {\n    domain: [0, 100],\n    grid: true,\n    label: \"Share of answer that cite a correct document (%)\"\n  },\n  x: {\n    label: \"Number of retrieved documents\",\n    type: \"linear\",\n  },\n  color: {\n    range: [\"#4758AB\", \"#ff562c\"]\n  },\n  marks: [\n    Plot.ruleY([0]),\n    Plot.line(topk, {\n      x: \"top\",\n      y: \"value\",\n      stroke: \"version\"\n    }),\n    Plot.dot(topk, {\n      x: \"top\",\n      y: \"value\",\n      stroke: \"version\",\n    }),\n    Plot.text(topk, Plot.selectLast({\n      x: \"top\",\n      y: \"value\",\n      z: \"version\",\n      text: \"version\",\n      textAnchor: \"start\",\n      dx: 4\n    }))\n  ],\n  width: 1100,\n  height: 400,\n  marginRight: 200,\n  style: {\n    fontWeight: \"bold\"\n  }\n})\n\n\n\n\n\n\n\nhtml`&lt;div class=\"custom\"&gt;${chart}&lt;style&gt;\n.custom svg { font-size: 20px !important }\n`"
  },
  {
    "objectID": "index.html#rag-behavior",
    "href": "index.html#rag-behavior",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "RAG behavior",
    "text": "RAG behavior\n\nHelped finding a satisfying prompt\n\n\nhallucination_data = [\n  {method: \"RAG ✅️\", value: 0.3145},\n  {method: \"RAG ❌️\", value: 78.33}\n]\n\n\n\n\n\n\n\nchart2 = Plot.plot({\n  marks: [\n    Plot.barY(hallucination_data, {\n      x: \"method\",\n      y: \"value\",\n      fill: \"#ff562c\"\n    }),\n    Plot.text(hallucination_data, {\n      x: \"method\",\n      y: \"value\",\n      text: d =&gt; d.value.toFixed(1) + \"%\",\n      dy: -8,\n      fontWeight: \"bold\"\n    })\n  ],\n  y: {\n    domain: [0, 100],\n    label: \"Share of answers with at least one invented reference (%)\"\n  },\n  width: 550,\n  height: 400,\n  fontWeight: \"bold\"\n})\n\n\n\n\n\n\n\nhtml`&lt;div class=\"custom\"&gt;${chart2}&lt;style&gt;\n.custom svg { font-size: 20px !important }\n`"
  },
  {
    "objectID": "index.html#first-user-feedbacks",
    "href": "index.html#first-user-feedbacks",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "First user feedbacks",
    "text": "First user feedbacks\n\nFirst feedbacks are mostly positive :\n\nStreaming is fast\n\nMain negative feedbacks:\n\nRetriever gives outdated papers (how to prioretize recent content?)\n\n\n\n\n\n\n\n\nRAG demo: rag-insee-demo-unece.lab.sspcloud.fr\n\n\nEasy to put into production thanks to:\n\nOur modular approach (Chroma or VLLM APIs)\nOur Kubernetes infrastructure SSPCloud"
  },
  {
    "objectID": "index.html#remaining-challenges",
    "href": "index.html#remaining-challenges",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Remaining challenges",
    "text": "Remaining challenges\n\nNeed to prioritize recent content\n\n\n\nNeed to prioritize national statistics unless question about specific area"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Conclusion",
    "text": "Conclusion\n\nRAG quality depends first (and IMO mostly) on how documents are parsed and processed\n\nBack to information retrieval problem !\nSee Barnett et al. (2024)\nLot of RAG resources focused on short documents…\n\n\n\n\nWe struggled for a long time because of poor technical choices\n\nOther choices (e.g. reranking, generative models…) can be handled after having a good pipeline"
  },
  {
    "objectID": "index.html#conclusion-1",
    "href": "index.html#conclusion-1",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "Conclusion",
    "text": "Conclusion\n\nllms.xt (llmstxt.org/): proposal to normalize website content for LLM ingestion\n\nMarkdown based approach\nSome big actors have adopted that norm\n\nAfter SEO, we will have GEO (Generative Engine Optimization)\n\nWe want an easy access to reliable information"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Challenges and insights in developing and evaluating RAG assistants",
    "section": "References",
    "text": "References\n\n\n\n\nBarnett, Scott, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, et Mohamed Abdelrazek. 2024. « Seven Failure Points When Engineering a Retrieval Augmented Generation System ». https://arxiv.org/abs/2401.05856.\n\n\nHusain, Hamel. 2024. « Your AI Product Needs Evals ». https://hamel.dev/blog/posts/evals/.\n\n\n———. 2025. « A Field Guide to Rapidly Improving AI Products ». https://hamel.dev/blog/posts/field-guide/."
  }
]